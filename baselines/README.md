# Baselines of the Metric Track

We provide the [deep AM-FM](https://link.springer.com/chapter/10.1007/978-981-15-8395-7_5) baseline system. Below is an incomplete list of useful repositories for automatic dialogue evaluation: 
- https://github.com/Maluuba/nlg-eval
- https://github.com/ZHAOTING/dialog-processing
- https://github.com/Shikib/usr
- https://github.com/li3cmz/GRADE
- https://github.com/exe1023/DialEvalMetrics
- https://github.com/Shikib/fed
- https://github.com/James-Yip/QuantiDCE
- https://github.com/alexzhou907/dialogue_evaluation
- https://github.com/e0397123/D-score
- https://github.com/e0397123/DynaEval
- https://github.com/Tiiiger/bert_score
- https://github.com/google-research/bleurt
- https://github.com/facebookresearch/online_dialog_eval
- https://github.com/prakharguptaz/multirefeval
- https://github.com/PlusLabNLP/PredictiveEngagement
- https://github.com/harsh19/Diverse-Reference-Augmentation/
- https://github.com/prakharguptaz/multirefeval
- https://github.com/harsh19/Diverse-Reference-Augmentation/

For comprehensive surveys, please refer to:
- [Survey on evaluation methods for dialogue systems, Deriu et al., 2021](https://link.springer.com/article/10.1007/s10462-020-09866-x)
- [A Comprehensive Assessment of Dialog Evaluation Metrics, Yeh et al., 2021](https://arxiv.org/abs/2106.03706)
- [A Survey of Evaluation Metrics Used for NLG Systems, Sai et al., 2020](https://arxiv.org/abs/2008.12009)
